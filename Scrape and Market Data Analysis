!pip install unidecode
!pip install -U kaleido # need to restart

# General Libraries
import os
import numpy as np
import pandas as pd
import nltk
import re
import unidecode
nltk.download('punkt')
from scipy import interpolate
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA
from scipy.interpolate import interp1d
import kaleido


# For Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# For Scrapping
import requests
import sqlalchemy
import requests
from bs4 import BeautifulSoup
from warnings import warn
import re

# Other
from matplotlib.ticker import MaxNLocator
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from IPython.display import display, HTML
from scipy.optimize import curve_fit
from numpy.polynomial.polynomial import Polynomial


#Connect Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define Export/Import Path
path = '/content/drive/MyDrive/Exports Cineuropa/'
date="09092024" # Update this to name new exported files

## 2 - Retreive Publicly Available Data (LUMIERE and IMDb)

# Store Data

id=[]
title=[]
year=[]
production_countries=[]
directors=[]
total_admissions_obs=[]
total_admissions_eu28 = []

# Identify the 44 European countries of the LUMIERE Database

countries=["AT","BA","BE","BG","BY","CH","CY","CZ","DE","DK","EE","ES","FI","FR","GB","GR","HR","HU","IE","IC","IT","LI","LT","LU","LV","MC","MD","ME","MK","MT","NL","NO","PL","PT","RO","RS","RU","SE","SI","SK","TR","RU","UA","XK",]

# Retreive data for films from LUMIERE. Results may vary according to updates in the database.
# Fourty-four countries of production, limited to the first 200 titles, per year, per country (exlcudes a few films in the most producting countries such as France).

cookies = {
    'dtCookie': 'v_4_srv_7_sn_C3DA38C1DDFF57CA84C15FE0062DFEC7_perc_100000_ol_0_mul_1_app-3A36637b77c52b10ae_1_rcs-3Acss_1',
}

headers = {
    'Accept': 'application/json, text/*',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'Content-Type': 'application/json; charset=UTF-8',
    #'Content-Length':"1000",
    'Origin': 'https://lumiere.obs.coe.int',
    'Referer': 'https://lumiere.obs.coe.int/search',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
}

for x in countries:
  for i in range(2001,2023):
    #i_1=str(i)
    i_2=str(i+1)
    data = '{"prod_start_year":2002,"prod_end_year":2002,"exp_start_year":1996,"exp_end_year":2023,"include_minority_coproducing_country":true,"production_country":"FR"}'
    data2=data.replace("2002",i_2)
    data3=data2.replace("FR",x)
    print(x,i_2)

    try:
      response = requests.post('https://lumiere.obs.coe.int/api/movies', headers=headers, cookies=cookies, data=data3)

      results_movies=response.json()

      for  result in results_movies:

        id.append(result["id"])

        title.append(result["original_title"])

        year.append(result["prod_year"])

        production_countries.append(result['production_countries'])

        directors.append(result['directors'])

        total_admissions_obs.append(result['total_admissions_obs'])

    except Exception as e:
      id.append(np.nan)

      title.append(np.nan)

      year.append(np.nan)

      production_countries.append(np.nan)

      directors.append(np.nan)

      total_admissions_obs.append(np.nan)

# Verify the number of unique ID (every film is associated to a single ID in the Lumiere Database) Numbers may vary following updates on the LUMIERE database
print(len(id))
print(any(np.isnan(value) if isinstance(value, (int, float)) else False for value in id))
print(len(list(set(id))))

# Create a Dataframe with the retreived values
movies_df=pd.DataFrame({'ID':id,'title':title,'year':year,'country':production_countries,"director":directors,'admissions':total_admissions_obs})

# Drop any duplicates
movies_df=movies_df.drop_duplicates(subset=['ID'])
movies_df

# Export Database
movies_df.to_excel(path+'moviesOBS'+date+'.xlsx', index=False)

## 3 - Further Data Analysis






### 3.1 - Import Data


#Import existing DataFrame with the sentiment score of critical reviews (Cineuropa, Variety, Hollywood Reporter, Screen Daily)
df = pd.read_csv(path+'MasterSentiment03012023.csv')
df

obs_all=movies_df

#Import DataFrame with Films with Market and Festival Data extracted from LUMIERE (To use if section 2 was completed in a different session)
obs_all = pd.read_excel(path+'moviesOBS'+date+'.xlsx')
obs_all

### 3.2 - Calculate the Yearly Film Production and the Yearly Admissions

# Calculate the total number of films released in the 44 European countries each year and the yearly admissions

s = pd.DataFrame()
count=-1
for specific_value in [2003, 2004, 2005, 2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023]:
  count+=1
# Calculate the sum of values in column "year" for rows where  has the specific value
  sum_for_specific_value = obs_all.loc[obs_all['year'] == specific_value, 'admissions'].sum()
  sum_films=(obs_all['year'] == specific_value).sum()
  s.at[count,'year']=specific_value
  s.at[count,'sum_films']=sum_films
  s.at[count,'sum_admissions']=sum_for_specific_value
  print(specific_value,sum_films,sum_for_specific_value)

# Calculate the sum of films per country of production (first country)
for two_letters_prefix in ["AT","BA","BE","BG","BY","CH","CY","CZ","DE","DK","EE","ES","FI","FR","GB","GR","HR","HU","IE","IC","IT","LI","LT","LU","LV","MC","MD","ME","MK","MT","NL","NO","PL","PT","RO","RS","RU","SE","SI","SK","TR","RU","UA","XK",]:

# Count the values in 'Country' that start with the specified two-letter string
  count_values_starting_with_prefix = (obs_all['country'].str.startswith(two_letters_prefix)).sum()
  print(two_letters_prefix,count_values_starting_with_prefix)

s_2=pd.DataFrame()
count=-1
for two_letters_prefix in ["AT","BA","BE","BG","BY","CH","CY","CZ","DE","DK","EE","ES","FI","FR","GB","GR","HR","HU","IE","IC","IT","LI","LT","LU","LV","MC","MD","ME","MK","MT","NL","NO","PL","PT","RO","RS","RU","SE","SI","SK","TR","RU","UA","XK",]:
  for specific_value in [2003, 2004, 2005, 2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023]:
    count+=1
    count_values_starting_with_prefix = (obs_all.loc[obs_all['year'] == specific_value, 'country'].str.startswith(two_letters_prefix)).sum()
    s_2.at[count,'country']=two_letters_prefix
    s_2.at[count,'year']=specific_value
    s_2.at[count,'sum_films']=count_values_starting_with_prefix

# Export the two dataframes (sums and sumspercountry)

s.to_excel(path+'sums'+date+'.xlsx', index=False)

s_2.to_excel(path+'sumspercountry'+date+'.xlsx', index=False)

### 3.3 - Conforming Data for Merge

# Conform texts

def clean_text(text):
    # Remove punctuation
    text_no_punct = re.sub(r'[^\w\s]', '', text)
    # Remove double spaces
    text_no_double_space = re.sub(r'\s+', ' ', text_no_punct).strip()

    return text_no_double_space

# Rename the necessary columns

obs_all = obs_all.rename(columns={"title":"original_title",
                                  #"director_x":"director",
                                  #"Year_x":"year",
                                  #"Countries_x":"countries",
                                  #"Total Admissions_x":"admissions",
                                  #"IMDB_x":"IMDB",
                                  #"Title imdb":"title"
                                  })

### 3.4 - Merge on the Basis of the Film's Original Title

# Drop NaN values for the column that will be the reference for merging

obs_all.dropna(subset=['original_title'], inplace=True)

obs_all['original_title'] = obs_all['original_title'].apply(lambda x: ' '.join(nltk.word_tokenize(x.lower())))
obs_all['original_title'] = obs_all['original_title'].apply(lambda x: unidecode.unidecode(x))
obs_all['original_title'] = obs_all['original_title'].apply(clean_text)

df['original_title'] = df['original_title'].apply(lambda x: ' '.join(nltk.word_tokenize(x.lower())))
df['original_title'] = df['original_title'].apply(lambda x: unidecode.unidecode(x))
df['original_title'] = df['original_title'].apply(clean_text)

title_merged = df.merge(obs_all, on=['original_title',"year"], how="left")
title_merged

# Check for missing market data (films with Cineuropa reviews that do not appear on the LUMIERE database)

title_merged['admissions'].isna().sum().sum()

### 3.5 - Merge on the basis of the film's Director and Year

# Fill in for missing values in the column that will be used as reference in merging
obs_all["director"]=obs_all["director"].fillna(value="")

obs_all['director'] = obs_all['director'].apply(lambda x: ' '.join(nltk.word_tokenize(x.lower())))
obs_all['director'] = obs_all['director'].apply(lambda x: unidecode.unidecode(x))
obs_all['director'] = obs_all['director'].apply(clean_text)

df['director'] = df['director'].apply(lambda x: ' '.join(nltk.word_tokenize(x.lower())))
df['director'] = df['director'].apply(lambda x: unidecode.unidecode(x))
df['director'] = df['director'].apply(clean_text)

director_merged=df.merge(obs_all, on=['director','year'], how='left')
director_merged

# Check for missing market data (films with Cineuropa reviews that do not appear on the LUMIERE database)

director_merged['admissions'].isna().sum().sum()

## 4 - Create and Export a Master Dataframe


### 4.1 - Prepare Data for Further Analysis

director_merged.keys()

columns_to_drop=['Unnamed: 0',
                 'ID_x',
                 'url',
                 'cineuropa_review_text',
                 'variety_review_text',
                 'hollywoodreporter_review_text',
                 'screendaily_review_text',
                 'country_y',
                 'original_title_y',]

director_merged=director_merged.drop(columns=columns_to_drop)
director_merged = director_merged.rename(columns={"original_title_x":"original_title",
                                                  #"title_x":"title",
                                                  "country_x": 'country',
                                                  'ID_y':'ID'
                                                  })

title_merged.keys()

columns_to_drop=['Unnamed: 0',
                 'ID_x',
                 'url',
                 'cineuropa_review_text',
                 'variety_review_text',
                 'hollywoodreporter_review_text',
                 'screendaily_review_text',
                 'country_y',
                 'director_y']

title_merged=title_merged.drop(columns=columns_to_drop)
title_merged = title_merged.rename(columns={"director_x":"director",
                                                  #"title_x":"title",
                                                  "country_x": 'country',
                                                  "ID_y":"ID"
                                                  })

prov_merged = title_merged.merge(director_merged, on=['original_title','year'], how='left')
prov_merged

prov_merged.keys()

prov_merged["admissions_final"]=prov_merged['admissions_x'].combine_first(prov_merged['admissions_y'])

prov_merged["admissions_final"].isna().sum().sum()

prov_merged["ID_final"]=prov_merged['ID_x'].combine_first(prov_merged['ID_y'])

prov_merged['ID_final'].isna().sum().sum()

columns_to_drop=['ID_x', 'admissions_x', 'title_y',
       'director_y', 'country_y', 'cineuropa_review_author_y',
       'cineuropa_review_date_y', 'variety_review_author_y',
       'variety_review_date_y', 'hollywoodreporter_review_author_y',
       'hollywoodreporter_review_date_y', 'screendaily_review_author_y',
       'screendaily_review_date_y', 'rottentomatoes_tomatometer_score_y',
       'rottentomatoes_audience_score_y', 'Predicted Sentiment Cineuropa_y',
       'Predicted Sentiment Variety_y',
       'Predicted Sentiment Hollywood Reporter_y',
       'Predicted Sentiment Screen Daily_y', 'ID_y', 'admissions_y',
        'variety_review_author_x', 'variety_review_date_x',
       'hollywoodreporter_review_author_x', 'hollywoodreporter_review_date_x',
       'screendaily_review_author_x', 'screendaily_review_date_x']
master_df=prov_merged.drop(columns=columns_to_drop)

master_df = master_df.rename(columns={"director_x":"director",
                                                  "title_x":"title",
                                                  "country_x": 'country',
                                                  "ID_final":"ID",
                                                  "country_x":"country",
                                                  'Predicted Sentiment Screen Daily_x':"Predicted Sentiment Screen Daily",
                                                  'Predicted Sentiment Hollywood Reporter_x':"Predicted Sentiment Hollywood Reporter",
                                                 'Predicted Sentiment Cineuropa_x':"Predicted Sentiment Cineuropa",
                                                  'Predicted Sentiment Variety_x':"Predicted Sentiment Variety",
                                      "rottentomatoes_tomatometer_score_x":'rottentomatoes_tomatometer_score',
                                      'rottentomatoes_audience_score_x':'rottentomatoes_audience_score'
                                                  })

master_df.keys()

master_df

# Export the first dataframe
master_df.to_excel(path+'master'+date+'.xlsx', index=False)

### 4.2 - Scrape the Festival Selection from IMDb and Distribution from LUMIERE

# Import if previous section was ran at a different session
test=pd.read_excel(path+'master'+date+'.xlsx')

new_df=master_df.dropna(subset=['ID'])

duplicates = new_df[new_df.duplicated(subset='ID', keep=False)]

# Print any duplicates found
if not duplicates.empty:
    print(f"Found duplicates:\n{duplicates}")
else:
    print("No duplicates found.")

# Drop dublicates

new_df = new_df.drop_duplicates(subset='ID', keep='first').reset_index(drop=True)

new_df["distr"] = np.nan
new_df["sum_distr"] = np.nan

# BUG: the first time we run this, index==0 is not retreived. Run it once only for index == 0 and then modify for the full database

# Iterate through the DataFrame
for index, row in new_df.iterrows():
    #if index != 0:
        #continue  # Skip all other rows except index (for mannual verification verify)

    countries = []
    i = row['ID']

    final_url = f'https://lumiere.obs.coe.int/api/work/{int(i)}/admissions'
    referer = f'https://lumiere.obs.coe.int/movie/{int(i)}'

    cookies = {
        'dtCookie': 'v_4_srv_7_sn_C3DA38C1DDFF57CA84C15FE0062DFEC7_perc_100000_ol_0_mul_1_app-3A36637b77c52b10ae_1_rcs-3Acss_1',
    }

    headers = {
        'Accept': 'application/json, text/*',
        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
        'Referer': referer,
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'sec-ch-ua': '^\\^Not_A',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '^\\^Windows^\\^',
    }

    try:
        # Make the request
        response = requests.get(final_url, headers=headers, cookies=cookies)
        response.raise_for_status()  # Raise an HTTPError for bad responses

        # Parse the response and retrieve country codes
        for entry in response.json():
            countries.append(entry["country_code"])

        # Keep only unique countries
        unique_countries = list(set(countries))

        # Assign the number of unique countries and the list to the DataFrame
        new_df.at[index, "sum_distr"] = len(unique_countries)
        new_df.at[index, "distr"] = unique_countries

    except Exception as e:
        # In case of any exception, set the values to NaN and print the error for debugging
        new_df.at[index, "sum_distr"] = e
        new_df.at[index, "distr"] = e
        print(f"Error retrieving data for ID {i}: {e}")


new_df

# Retreive IMDb links for each film from Lumiere
# BUG: the first time we run this, index==0 is not retreived. Run it once only for index == 0 and then modify for the full database


url='https://lumiere.obs.coe.int/api/movie/'
referer='https://lumiere.obs.coe.int/movie/'

count=-1
for i in new_df["ID"]:
  final_url=url+str(int(float(i)))
  final_referer=referer+str(int(float(i)))

  cookies = {
      'dtCookie': 'v_4_srv_7_sn_C3DA38C1DDFF57CA84C15FE0062DFEC7_perc_100000_ol_0_mul_1_app-3A36637b77c52b10ae_1_rcs-3Acss_1',
  }

  headers = {
      'Accept': 'application/json, text/*',
      'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
      'Connection': 'keep-alive',
      'Referer': referer,
      'Sec-Fetch-Dest': 'empty',
      'Sec-Fetch-Mode': 'cors',
      'Sec-Fetch-Site': 'same-origin',
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
      'sec-ch-ua': '^\\^Not_A',
      'sec-ch-ua-mobile': '?0',
      'sec-ch-ua-platform': '^\\^Windows^\\^',
  }

  response2 = requests.get(final_url, headers=headers, cookies=cookies)
  try:
    new_df.loc[new_df["ID"]==i, "IMDB"] = response2.json()['links'][0]['url']

  except Exception as e:
    new_df.loc[new_df["ID"]==i, "IMDB"]=e


new_df['IMDB'].isna().sum()

new_df.to_excel(path+'films_noimdb'+date+'.xlsx',index=False)

# Import if previous session has expired
new_df=pd.read_excel(path+'films_noimdb'+date+'.xlsx')

new_df

# This execution may take several hours
# BUG: the first time we run this, index==0 is not retreived. Run it once only for index == 0 and then modify for the full database


# Set headers
headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:101.0) Gecko/20100101 Firefox/101.0"}

# Iterate through the DataFrame using iterrows()
for index, row in new_df.iterrows():
    imdb_link = row['IMDB']
    imdb_festivals = []

    # Check if the IMDB link is valid (not NaN)
    if isinstance(imdb_link, str):
        final_url = imdb_link + "awards"

        try:
            # Make the requests
            source = requests.get(final_url, headers=headers)
            source2 = requests.get(imdb_link, headers=headers)

            # Parse the HTML
            soup = BeautifulSoup(source.text, 'html.parser')
            soup2 = BeautifulSoup(source2.text, 'html.parser')

            # Retrieve awards information
            try:
                awards_prov = soup.find('div', attrs={'data-testid': 'awards-signpost'}).text
                new_df.at[index, "awards"] = awards_prov
            except Exception:
                new_df.at[index, "awards"] = np.nan

            # Retrieve festival information
            try:
                festivals_prov = soup.find_all('h3', attrs={'class': 'ipc-title__text'}) # note that this code is subject to change when IMDb updates
                for element in festivals_prov:
                    imdb_festivals.append(element.get_text())
                # Assign the list of festivals to the DataFrame (excluding last 4 irrelevant entries)
                new_df.at[index, "festivals"] = imdb_festivals[:-4]
            except Exception as e:
                new_df.at[index, "festivals"] = str(e)

            # Retrieve IMDb score
            try:
                score_prov = soup2.find(attrs={"class": "sc-eb51e184-1 ljxVSS"}).text # note that this code is subject to change when IMDb updates
                new_df.at[index, "imdb_score"] = score_prov
            except Exception as e:
                new_df.at[index, "imdb_score"] = str(e)

        except Exception as e:
            # Handle any request or parsing errors
            new_df.at[index, "festivals"] = str(e)

new_df.to_excel(path+'films_withimdb'+date+'.xlsx', index=False)

### 4.3 - Calculate the Festival Variable

# Import the master DataFrame (run if previous cells were executed at a different session)
new_df=pd.read_excel(path+"films_withimdb"+date+".xlsx")

# List of A-rank Festivals according to the FIAPF

list_festivals=['Fajr International Film Festival',
                "Berlin International Film Festival",
                "Cannes	Cannes Film Festival",
                "Shanghai International Film Festival",
                "Karlovy Vary International Film Festival",
                "Locarno International Film Festival",
                "Montreal World Film Festival",
                "Venice Film Festival",
                "San Sebastián International Film Festival",
                "Warsaw International Film Festival",
                "Tokyo International Film Festival",
                "Mar del Plata Film Festival",
                "Tallinn Black Nights Film Festival",
                "Cairo International Film Festival",
                "International Film Festival of India",
                "Rome Film Festival",
                "Vienna International Film Festival",
                "Viennale",
                "Toronto International Film Festival",
                "Kraków Film Festival",
                "Bilbao International Festival of Documentary and Short Films",
                "International Short Film Festival Oberhausen",
                "Tampere Film Festival",
                "Busan International Film Festival",
                "Cinedays Film Festival",
                "Festival de Cine Global de Santo Domingo",
                "Lisbon International Horror Film Festival",
                "Cartagena Film Festival",
                "Eurasia International Film Festival",
                "Gijón International Film Festival",
                "International Antalya Golden Orange Film Festival",
                "International Film Festival of Kerala",
                "Istanbul International Film Festival",
                "Kitzbühel Film Festival",
                "Kolkata International Film Festival",
                "Kyiv International Film Festival",
                "Mumbai Film Festival",
                "Noir in Festival",
                "Sitges Film Festival",
                "Sofia International Film Festival",
                "Sydney Film Festival",
                "Torino Film Festival",
                "Transilvania International Film Festival"]

new_df['festivals_rank'] = new_df['festivals'].apply(lambda x: any(value in x for value in list_festivals))

new_df['festival_values']=np.nan

condition = new_df['awards'].apply(lambda x: x if x is np.nan else False)

# Use np.where to create the new column
new_df['festival_values'] = np.where(condition, 0, 1)

new_df['festival_values'] = np.where(new_df['festivals_rank'] == 1.0, 2, new_df['festival_values'])

new_df.head(20)

new_df["festival_values"].value_counts()

### 4.4 - Assign a Market Size for each Film

Using the classification of the EAO Annual Reports

# Use the market cluster defined in the EAO to assign a market size for each film

country_list_s = ["Cyprus","Estonia",'Finland','Greece','Croatia','Iceland','Lithuania',
                  'Luxembourg','Latvia','Bosnia and Herzegovina','Georgia','Montenegro',
                  'North Macedonia','Malta','Slovenia','Slovak Republic',"Bulgaria","Serbia", "Slovakia","Moldova",'Albania']
country_list_m = ["Turkey",'Austria', 'Belgium','Switzerland','Portugal',"Denmark", "Hungary","Ireland","Netherlands","Romania","Norway","Sweden","Czechia","Czech Republic", "Ukraine"]
country_list_l = ['Germany', 'France', "Italy", "Spain","United Kingdom", "Poland"]

new_df['markets']=np.nan

def check_first_country(value):
    first_country = str(value).split(',')[0].strip()  # Get the first country and strip any whitespace

    if first_country in country_list_s:
        return 'S'
    elif first_country in country_list_m:
        return 'M'
    elif first_country in country_list_l:
        return 'L'
    else:
        # Fallback to the existing value in 'markets' if no match
        return new_df['markets'].loc[new_df['country'] == value].values[0] if value in new_df['country'].values else np.nan

# Apply the combined function to the 'country' column
new_df['markets'] = new_df['country'].apply(check_first_country)


new_df['markets'].value_counts()

new_df['markets'].isna().sum()

Using the classification of the EAO 2023 Report

new_df_2023 = new_df.copy()

country_list_ss = ["Estonia",'Croatia','Lithuania',
                  'Latvia','Slovenia','Slovak Republic',"Bulgaria", "Serbia", "Slovakia","Moldova",'Albania']

country_list_s = ['Austria', 'Belgium','Switzerland','Portugal',"Denmark", "Greece", "Finland", "Hungary","Romania","Norway","Sweden","Czechia","Czech Republic", "Ukraine"]

country_list_sm = ["Netherlands"]

country_list_m = ['Poland', 'Turkey']

country_list_ml = ['Germany', "Italy", "Spain"]

country_list_l = [ 'France',"United Kingdom","Ireland"]

def check_first_country(value):
    first_country = str(value).split(',')[0].strip()  # Get the first country and strip any whitespace

    if first_country in country_list_ss:
        return 'SS'
    elif first_country in country_list_s:
        return 'S'
    elif first_country in country_list_sm:
        return 'SM'
    elif first_country in country_list_m:
        return 'M'
    elif first_country in country_list_ml:
        return 'ML'
    elif first_country in country_list_l:
        return 'L'
    else:
        # Fallback to the value in 'markets' if the country doesn't match any list
        return new_df_2023['markets'].loc[new_df_2023['country'] == value].values[0] if value in new_df_2023['country'].values else np.nan

# Apply the combined function to the 'country' column
new_df_2023['markets'] = new_df_2023['country'].apply(check_first_country)


new_df_2023['markets'].value_counts()

new_df_2023['markets'].isna().sum()

new_df_2023=new_df_2023.dropna(subset=['markets'])
new_df=new_df.dropna(subset=['markets'])

# Export the updated master dataframe

new_df.to_excel(path+'master'+date+'.xlsx', index=False)

# Export the updated master dataframe

new_df_2023.to_excel(path+'master_2023_'+date+'.xlsx', index=False)

### 4.5 - Identify French Films

This section checks the country of production for each film and identifies where french productions were distributed outside France

# Rename the DatFrame
df=new_df

# Initialize columns
df['French productions'] = np.nan
df['Distribution in France'] = np.nan
df['FR Penetration'] = np.nan

# Combined function for French production status
def process_row(countries, distr):
    # Process French production status
    countries_list = [country.strip() for country in countries.split(',')]
    french_production = 'Majority' if countries_list[0] == 'France' else 'Minority' if 'France' in countries_list else 'No'

    # Process French distribution status
    french_distribution = 'Yes' if 'FR' in distr else 'No'

    # Process FR penetration
    fr_penetration = 'only FR' if distr == "['FR']" else 'includes FR' if 'FR' in distr else 'no FR'

    return french_production, french_distribution, fr_penetration

# Apply the combined function to the dataframe
df[['French productions', 'Distribution in France', 'FR Penetration']] = df.apply(
    lambda row: process_row(row['country'], row['distr']), axis=1, result_type='expand'
)


df

# Print the results
condition_count1 = ((df['French productions'] == 'Majority') & (df['FR Penetration'] == 'only FR')).sum()
print(f"\nNumber of French majority productions distributed only in France: {condition_count1}")

condition_count2 = ((df['French productions'] == 'Minority') & (df['FR Penetration'] == 'only FR')).sum()
print(f"\nNumber of French minority productions distributed only in France: {condition_count2}")

condition_count3 = ((df['French productions'] == 'Majority') & (df['FR Penetration'] == 'no FR')).sum()
print(f"\nNumber of French majority productions not distributed in France: {condition_count3}")

condition_count4 = ((df['French productions'] == 'Minority') & (df['FR Penetration'] == 'no FR')).sum()
print(f"\nNumber of French minority productions not distributed in France: {condition_count4}")

condition_count5 = ((df['French productions'] == 'Majority') & (df['FR Penetration'] == 'includes FR')).sum()
print(f"\nNumber of French majority productions distributed both in France and abroad: {condition_count5}")

condition_count6 = ((df['French productions'] == 'Minority') & (df['FR Penetration'] == 'includes FR')).sum()
print(f"\nNumber of French minority productions distributed both in France and abroad: {condition_count6}")

condition_count7 = ((df['French productions'] == 'Majority')).sum()
print(f"\nNumber of French majority productions {condition_count7}")

condition_count8 = ((df['French productions'] == 'Minority')).sum()
print(f"\nNumber of French minority productions {condition_count8}")

print(f"\npercentage of French majority productions with EU distribution: {(condition_count5/condition_count7)*100} %")

print(f"\npercentage of French productions (including minority) with EU distribution: {((condition_count5+condition_count6)/(condition_count7+condition_count8))*100} %")


# Export new DataFrame
df.to_excel(path+'master_FR'+date+".xlsx", index=False)

## 5 - Calculate the Thresholds for Commercial Success

This section gathers the necessary market data to calculate the tresholds for commercial success for every market and every year

### 5.1 - Calculate the Average Sentiment Score for Industry Outlets

# Rename the DataFrame
df=new_df

df

# Ensure no chained indexing occurs before this point and use loc for safe assignment.

# Calculate the industry average using loc
df['industry_average'] = df.loc[:, ['Predicted Sentiment Variety', 'Predicted Sentiment Hollywood Reporter', 'Predicted Sentiment Screen Daily']].mean(axis=1, skipna=True)

# Calculate the industry low using loc
df['industry_low'] = df.loc[:, ['Predicted Sentiment Variety', 'Predicted Sentiment Hollywood Reporter', 'Predicted Sentiment Screen Daily']].min(axis=1, skipna=True)


nan_count = df['Predicted Sentiment Cineuropa'].isna().sum()
print(f'Total NaN values in Predicted Sentiment Cineuropa: {nan_count}')

 ### 5.2 - Estimate the Yearly Average Ticket Prices per Market Cluster

# Import data on ticket price, from the EAO publication at the 2022 Cannes film festival
path = '/content/drive/MyDrive/Exports Cineuropa/'
tickets=pd.read_excel(path+'ticketspercountry2021.xlsx')
tickets

# The countries in the three market clusters according to the EAO annual reports
country_list_s = ["Cyprus","Estonia",'Finland','Greece','Croatia','Iceland','Lithuania',
                  'Luxembourg','Latvia','Bosnia and Herzegovina','Georgia','Montenegro',
                  'North Macedonia','Malta','Slovenia','Slovakia',"Bulgaria"]
country_list_m = ['Austria', 'Belgium','Switzerland','Portugal',"Denmark", "Hungary","Ireland","Netherlands","Romania","Norway","Sweden","Czech Republic","Ukraine"]
country_list_l = ['Germany', 'France', "Italy", "Spain","United Kingdom", "Poland", "Russia", "United States", "Turkey","Canada", "China", "Brazil", "Japan"]


tickets['markets'] = tickets['Country'].apply(lambda value: 'S' if any(country in str(value) for country in country_list_s) else np.nan)
tickets['markets'] = tickets['Country'].apply(lambda value: 'M' if any(country in str(value) for country in country_list_m) else tickets['markets'].loc[tickets['Country'] == value].values[0] if value in tickets['Country'].values else np.nan)
tickets['markets'] = tickets['Country'].apply(lambda value: 'L' if any(country in str(value) for country in country_list_l) else tickets['markets'].loc[tickets['Country'] == value].values[0] if value in tickets['Country'].values else np.nan)

average_ticket_for_S = tickets.loc[tickets['markets'] == 'S', 'Ticket'].mean()
average_ticket_for_M = tickets.loc[tickets['markets'] == 'M', 'Ticket'].mean()
average_ticket_for_L = tickets.loc[tickets['markets'] == 'L', 'Ticket'].mean()
print(f'Average ticket prices for 2021 (S, M, L):',round(average_ticket_for_S,2), round(average_ticket_for_M,2),round(average_ticket_for_L,2))

# Import existing DataFrame with annual HICP values
hicp_pd =pd.read_excel(path+'hicp.xlsx')
hicp_pd

# Round and convert the values to scalars, assigning them directly
hicp_pd.loc[hicp_pd["year"] == 2021, "average_ticket_s"] = float(round(average_ticket_for_S, 2))
hicp_pd.loc[hicp_pd["year"] == 2021, "average_ticket_m"] = float(round(average_ticket_for_M, 2))
hicp_pd.loc[hicp_pd["year"] == 2021, "average_ticket_l"] = float(round(average_ticket_for_L, 2))

hicp_pd

# Export the new table
hicp_pd.to_excel(path+'hicp'+date+'.xlsx', index=False)

We manually calculated the average ticket prices for the rest of the years according to the following formula:

*Price{year_n+1} = Price{year_n} / (1+HICP{year_n+1} / 100)*

# Import the new DataFrame after the manual calcuation

tickets=pd.read_excel(path+'inflation.xlsx')
tickets

### 5.3 - Estimate the Yearly Median Production Budget per Market Cluster

data_france = {
    'year': np.arange(2003, 2023),
    'budget_france': [2760000,
2940000,
3450000,
3060000,
2500000,
3160000,
3470000,
3100000,
2900000,
3390000,
3110000,
3420000,
2620000,
2960000,
3180000,
2720000,
2580000,
2560000,
2800000,
2950000
]
}
df_france = pd.DataFrame(data_france)


# Historical data for Market L (2016-2021)
data_l = {
    'year': np.arange(2016, 2022),
    'budget_L': [3300000, 3200000, 2800000, 3100000, 2700000, 2700000]
}
df_l = pd.DataFrame(data_l)

# Define the years where we need predictions
years_to_predict = np.concatenate([np.arange(2003, 2016), np.array([2022])])

interpolator_france = interp1d(df_france['year'], df_france['budget_france'], kind='linear', fill_value="extrapolate")

# Predict the budgets for Product B using the interpolation function based on Product A
predicted_budgets_france = interpolator_france(years_to_predict)

# Extract the years and budgets for Product B where you have actual data
years_l = df_l['year'].values
budgets_l = df_l['budget_L'].values

# Create interpolation function for Product B
interpolator_l = interp1d(years_l, budgets_l, kind='linear', fill_value="extrapolate")

# Predict the budgets for Product B for years 2016-2021 (to validate)
predicted_budgets_l_actual = interpolator_l(years_l)


overlap_years = np.intersect1d(years_l, df_france['year'])
if len(overlap_years) > 0:
    ratio = budgets_l[np.in1d(years_l, overlap_years)] / df_france['budget_france'][np.in1d(df_france['year'], overlap_years)]
    # Calculate average ratio
    average_ratio = np.mean(ratio)
else:
    average_ratio = 1  # Default ratio if no overlap (though this should not happen with given data)

print(f"Average Ratio of Market L to Prench films variation: {average_ratio:.4f}")

# Adjust Product A's predictions by this ratio to estimate Product B's values
predicted_budgets_l = predicted_budgets_france * average_ratio

# Print the predicted budgets for Product B using Product A's adjusted values
print("Predicted Budgets for Market L (adjusted based on French Films' behavior):")
for year, budget in zip(years_to_predict, predicted_budgets_l):
    print(f"Year: {year}, Predicted Budget: {budget:.2f}")

# Plotting the results

# Filter data for years after 2007
df_france_filtered = df_france[df_france['year'] > 2007]
df_l_filtered = df_l[df_l['year'] > 2007]
years_to_predict_filtered = [year for year in years_to_predict if year > 2007]
predicted_budgets_l_filtered = [budget for year, budget in zip(years_to_predict, predicted_budgets_l) if year > 2007]


plt.plot(df_france_filtered['year'], df_france_filtered['budget_france'], 'o', label='French Films (CNC data)', color='blue')
plt.plot(years_to_predict_filtered, predicted_budgets_l_filtered, 'x', label='Mean Prod. Budget - L Market (Predicted)', color='orange')
plt.plot(df_l_filtered['year'], df_l_filtered['budget_L'], 'o', label='L Market (EAO data)', color='green')
#plt.legend()
plt.xlabel('Year')
plt.ylabel('Budget (euros)')
plt.title('Median Prod. Budget Prediction for Large Market')

plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))

# Add a grid and display the plot
plt.grid(True)
plt.savefig('plot_market_L.pdf')
plt.show()

# Historical data for Market S (2016-2021)
data_s = {
    'year': np.arange(2016, 2022),
    'budget_S': [900000,900000,1200000,1100000,1100000,700000]
}
df_s = pd.DataFrame(data_s)

# Define the years where we need predictions
years_to_predict = np.concatenate([np.arange(2003, 2016), np.array([2022])])

interpolator_france = interp1d(df_france['year'], df_france['budget_france'], kind='linear', fill_value="extrapolate")

# Predict the budgets for Product B using the interpolation function based on Product A
predicted_budgets_france = interpolator_france(years_to_predict)

# Extract the years and budgets for Product B where you have actual data
years_s = df_s['year'].values
budgets_s = df_s['budget_S'].values

# Create interpolation function for Product B
interpolator_s = interp1d(years_s, budgets_s, kind='linear', fill_value="extrapolate")

# Predict the budgets for Product B for years 2016-2021 (to validate)
predicted_budgets_s_actual = interpolator_s(years_s)


overlap_years = np.intersect1d(years_l, df_france['year'])
if len(overlap_years) > 0:
    ratio = budgets_s[np.in1d(years_s, overlap_years)] / df_france['budget_france'][np.in1d(df_france['year'], overlap_years)]
    # Calculate average ratio
    average_ratio = np.mean(ratio)
else:
    average_ratio = 1  # Default ratio if no overlap (though this should not happen with given data)

print(f"Average Ratio of Market L to Prench films variation: {average_ratio:.4f}")

# Adjust Product A's predictions by this ratio to estimate Product B's values
predicted_budgets_s = predicted_budgets_france * average_ratio

# Print the predicted budgets for Product B using Product A's adjusted values
print("Predicted Budgets for Market L (adjusted based on French Films' behavior):")
for year, budget in zip(years_to_predict, predicted_budgets_s):
    print(f"Year: {year}, Predicted Budget: {budget:.2f}")

# Plotting the results

# Filter data for years after 2007
df_france_filtered = df_france[df_france['year'] > 2007]
df_s_filtered = df_s[df_l['year'] > 2007]
years_to_predict_filtered = [year for year in years_to_predict if year > 2007]
predicted_budgets_s_filtered = [budget for year, budget in zip(years_to_predict, predicted_budgets_s) if year > 2007]


plt.plot(df_france_filtered['year'], df_france_filtered['budget_france'], 'o', label='French Films (CNC data)', color='blue')
plt.plot(years_to_predict_filtered, predicted_budgets_l_filtered, 'x', label='Mean Prod. Budget - S Market (Predicted)', color='orange')
plt.plot(df_s_filtered['year'], df_s_filtered['budget_S'], 'o', label='S Market (EAO data)', color='green')
#plt.legend()
plt.xlabel('Year')
plt.ylabel('Budget (euros)')
plt.title('Median Prod. Budget Prediction for Small Market')

plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))

# Add a grid and display the plot
plt.grid(True)
plt.savefig('plot_market_S.pdf')
plt.show()# Historical data for Market S (2016-2021)

# Historical data for Market M (2016-2021)
data_m = {
    'year': np.arange(2016, 2022),
    'budget_M': [1600000, 1500000,1700000,1600000,1700000,1800000]
}
df_m = pd.DataFrame(data_m)

# Define the years where we need predictions
years_to_predict = np.concatenate([np.arange(2003, 2016), np.array([2022])])

interpolator_france = interp1d(df_france['year'], df_france['budget_france'], kind='linear', fill_value="extrapolate")

# Predict the budgets for Product B using the interpolation function based on Product A
predicted_budgets_france = interpolator_france(years_to_predict)

# Extract the years and budgets for Product B where you have actual data
years_m = df_m['year'].values
budgets_m = df_m['budget_M'].values

# Create interpolation function for Product B
interpolator_m = interp1d(years_m, budgets_m, kind='linear', fill_value="extrapolate")

# Predict the budgets for Product B for years 2016-2021 (to validate)
predicted_budgets_m_actual = interpolator_m(years_m)


overlap_years = np.intersect1d(years_m, df_france['year'])
if len(overlap_years) > 0:
    ratio = budgets_m[np.in1d(years_m, overlap_years)] / df_france['budget_france'][np.in1d(df_france['year'], overlap_years)]
    # Calculate average ratio
    average_ratio = np.mean(ratio)
else:
    average_ratio = 1  # Default ratio if no overlap (though this should not happen with given data)

print(f"Average Ratio of Market M to Prench films variation: {average_ratio:.4f}")

# Adjust Product A's predictions by this ratio to estimate Product B's values
predicted_budgets_m = predicted_budgets_france * average_ratio

# Print the predicted budgets for Product B using Product A's adjusted values
print("Predicted Budgets for Market M (adjusted based on French Films' behavior):")
for year, budget in zip(years_to_predict, predicted_budgets_s):
    print(f"Year: {year}, Predicted Budget: {budget:.2f}")

# Plotting the results

# Filter data for years after 2007
df_france_filtered = df_france[df_france['year'] > 2007]
df_m_filtered = df_m[df_l['year'] > 2007]
years_to_predict_filtered = [year for year in years_to_predict if year > 2007]
predicted_budgets_m_filtered = [budget for year, budget in zip(years_to_predict, predicted_budgets_m) if year > 2007]


plt.plot(df_france_filtered['year'], df_france_filtered['budget_france'], 'o', label='French Films (CNC data)', color='blue')
plt.plot(years_to_predict_filtered, predicted_budgets_m_filtered, 'x', label='Mean Prod. Budget - M Market (Predicted)', color='orange')
plt.plot(df_m_filtered['year'], df_m_filtered['budget_M'], 'o', label='M Market (EAO data)', color='green')
#plt.legend()
plt.xlabel('Year')
plt.ylabel('Budget (euros)')
plt.title('Median Prod. Budget Prediction for Medium Market')

plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))

# Add a grid and display the plot
plt.grid(True)
plt.savefig('plot_market_M.pdf')
plt.show()# Historical data for Market M (2016-2021)

predicted_s = pd.DataFrame({
    'year': years_to_predict,
    'predicted_budget_S': predicted_budgets_s
})


predicted_m = pd.DataFrame({
    'year': years_to_predict,
    'predicted_budget_M': predicted_budgets_m
})


predicted_l = pd.DataFrame({
    'year': years_to_predict,
    'predicted_budget_L': predicted_budgets_l
})


all_years = np.arange(2003, 2023)
df_all_years = pd.DataFrame({'year': all_years})


# Merge historical and predicted budgets into the all-years DataFrame
df_all_years = df_all_years.merge(df_s, on='year', how='left')  # Add historical data
df_all_years = df_all_years.merge(predicted_s, on='year', how='left')  # Add predicted data

# Fill the 'budget_B' column with the historical value where available, otherwise use the predicted value
df_all_years['budget_S'] = df_all_years['budget_S'].fillna(df_all_years['predicted_budget_S'])

# Drop the predicted_budget_B column as it is no longer needed
df_all_years = df_all_years.drop(columns='predicted_budget_S')



df_all_years = df_all_years.merge(df_m, on='year', how='left')  # Add historical data
df_all_years = df_all_years.merge(predicted_m, on='year', how='left')  # Add predicted data

# Fill the 'budget_B' column with the historical value where available, otherwise use the predicted value
df_all_years['budget_M'] = df_all_years['budget_M'].fillna(df_all_years['predicted_budget_M'])

# Drop the predicted_budget_B column as it is no longer needed
df_all_years = df_all_years.drop(columns='predicted_budget_M')


df_all_years = df_all_years.merge(df_l, on='year', how='left')  # Add historical data
df_all_years = df_all_years.merge(predicted_l, on='year', how='left')  # Add predicted data

# Fill the 'budget_B' column with the historical value where available, otherwise use the predicted value
df_all_years['budget_L'] = df_all_years['budget_L'].fillna(df_all_years['predicted_budget_L'])

# Drop the predicted_budget_B column as it is no longer needed
df_all_years = df_all_years.drop(columns='predicted_budget_L')

df_all_years

tickets_reversed = tickets[::-1].reset_index(drop=True)
tickets_reversed

tickets_reversed['average budget_s']= df_all_years['budget_S']
tickets_reversed['average budget_m']= df_all_years['budget_M']
tickets_reversed['average budget_l']= df_all_years['budget_L']

### 5.3 - Calculate and Export Thresholds (Our Method)

# Rename the DataFrame
inflation=tickets_reversed

# Using our conditions to caclulate each threshold

for count in range(0,20):
  micro_threshold_s= ((inflation.loc[count, 'average budget_s']/4) / inflation.loc[count, 'average_ticket_s'])
  micro_threshold_m= ((inflation.loc[count, 'average budget_m']/4) / inflation.loc[count, 'average_ticket_m'])
  micro_threshold_l= ((inflation.loc[count, 'average budget_l']/4) / inflation.loc[count, 'average_ticket_l'])
  inflation.loc[count, 'micro_threshold_s'] = micro_threshold_s
  inflation.loc[count, 'micro_threshold_m'] = micro_threshold_m
  inflation.loc[count, 'micro_threshold_l'] = micro_threshold_l

  low_threshold_s= ((inflation.loc[count, 'average budget_s']/2) / inflation.loc[count, 'average_ticket_s'])
  low_threshold_m= ((inflation.loc[count, 'average budget_m']/2) / inflation.loc[count, 'average_ticket_m'])
  low_threshold_l= ((inflation.loc[count, 'average budget_l']/2) / inflation.loc[count, 'average_ticket_l'])
  inflation.loc[count, 'low_threshold_s'] = low_threshold_s
  inflation.loc[count, 'low_threshold_m'] = low_threshold_m
  inflation.loc[count, 'low_threshold_l'] = low_threshold_l

  medium_threshold_s= (inflation.loc[count, 'average budget_s'] / inflation.loc[count, 'average_ticket_s'])
  medium_threshold_m= (inflation.loc[count, 'average budget_m'] / inflation.loc[count, 'average_ticket_m'])
  medium_threshold_l= (inflation.loc[count, 'average budget_l'] / inflation.loc[count, 'average_ticket_l'])
  inflation.loc[count, 'medium_threshold_s'] = medium_threshold_s
  inflation.loc[count, 'medium_threshold_m'] = medium_threshold_m
  inflation.loc[count, 'medium_threshold_l'] = medium_threshold_l

  high_threshold_s= ((inflation.loc[count, 'average budget_s']*1.5) / inflation.loc[count, 'average_ticket_s'])
  high_threshold_m= ((inflation.loc[count, 'average budget_m']*1.5) / inflation.loc[count, 'average_ticket_m'])
  high_threshold_l= ((inflation.loc[count, 'average budget_l']*1.5) / inflation.loc[count, 'average_ticket_l'])
  inflation.loc[count, 'high_threshold_s'] = high_threshold_s
  inflation.loc[count, 'high_threshold_m'] = high_threshold_m
  inflation.loc[count, 'high_threshold_l'] = high_threshold_l

classification=inflation

classification.to_excel(path+"classification"+date+".xlsx", index=False)

### 5.4 - Calculate and Export Thresholds (EAO 2023 Report Method)

# Using the thresholds published in the EAO 2023 Report

classification_2022report = pd.DataFrame()

micro_threshold_ss= 20000
micro_threshold_s= 35000
micro_threshold_sm= 100000
micro_threshold_m = 150000
micro_threshold_ml= 250000
micro_threshold_l = 350000

low_threshold_ss= 50000
low_threshold_s= 130000
low_threshold_sm= 200000
low_threshold_m = 400000
low_threshold_ml= 800000
low_threshold_l = 1500000

medium_threshold_ss= 100000
medium_threshold_s= 270000
medium_threshold_sm= 500000
medium_threshold_m = 950000
medium_threshold_ml= 1800000
medium_threshold_l = 3000000

years = list(range(2002, 2022))

# Create the 'year' column
classification_2022report['year'] = years

# Add a column 'micro_threshold_ss' with the same value 1 for all years
classification_2022report['micro_threshold_ss'] = micro_threshold_ss
classification_2022report['micro_threshold_s'] = micro_threshold_s
classification_2022report['micro_threshold_sm'] = micro_threshold_sm
classification_2022report['micro_threshold_m'] = micro_threshold_m
classification_2022report['micro_threshold_ml'] = micro_threshold_ml
classification_2022report['micro_threshold_l'] = micro_threshold_l

classification_2022report['low_threshold_ss'] = low_threshold_ss
classification_2022report['low_threshold_s'] = low_threshold_s
classification_2022report['low_threshold_sm'] = low_threshold_sm
classification_2022report['low_threshold_m'] = low_threshold_m
classification_2022report['low_threshold_ml'] = low_threshold_ml
classification_2022report['low_threshold_l'] = low_threshold_l

classification_2022report['medium_threshold_ss'] = medium_threshold_ss
classification_2022report['medium_threshold_s'] = medium_threshold_s
classification_2022report['medium_threshold_sm'] = medium_threshold_sm
classification_2022report['medium_threshold_m'] = medium_threshold_m
classification_2022report['medium_threshold_ml'] = medium_threshold_ml
classification_2022report['medium_threshold_l'] = medium_threshold_l

classification_2022report.to_excel(path+"classification_2023report"+date+".xlsx", index=False)

## 6 - Calculate Ratio of Success

This section calculates how many films are classified as successful each year (both our method and the one from the EAO 2023 Report) and their ration compared with the annual production

### 6.1 - Calculate Ratio of Success

# Import data

path = '/content/drive/MyDrive/Exports Cineuropa/'
df = pd.read_excel(path+'master0402024.xlsx') # Baseline
df2022 = pd.read_excel(path+'master_2022report_24072024.xlsx') #Report 2023
classification_2022report = pd.read_excel(path+'classification_2022report_24072024.xlsx') # Robustness checks
classification = pd.read_excel(path+'classification_24072024.xlsx')

# Import the full database retreived from LUMIERE (before exlcuing the non-reviewed films)
obs = pd.read_excel(path+'moviesOBS'+date+'.xlsx')
obs



# Prepare data for further analysis

df_no_na = df.dropna(subset=['markets', 'admissions_final', 'Predicted Sentiment Cineuropa']).reset_index(drop=True)
df_no_na_2022 = new_df_2023.dropna(subset=['markets', 'admissions_final', 'Predicted Sentiment Cineuropa']).reset_index(drop=True)


classification.rename(columns ={'Year': 'year'}, inplace=True)
classification_2022report.rename(columns={'Year': 'year'}, inplace=True)

df_no_na = df.dropna(subset=['markets', 'admissions_final', 'Predicted Sentiment Cineuropa']).reset_index(drop=True)

df_no_na

classification=inflation

2023 report classification method

def classify_films2022(df, classification):
    # Define the threshold names for each market size
    thresholds = {
        'SS': ['micro_threshold_ss', 'low_threshold_ss', 'medium_threshold_ss'],
        'S': ['micro_threshold_s', 'low_threshold_s', 'medium_threshold_s'],
	      'SM': ['micro_threshold_sm', 'low_threshold_sm', 'medium_threshold_sm'],
        'M': ['micro_threshold_m', 'low_threshold_m', 'medium_threshold_m'],
	      'ML': ['micro_threshold_ml', 'low_threshold_ml', 'medium_threshold_ml'],
	      'L': ['micro_threshold_l', 'low_threshold_l', 'medium_threshold_l'],
    }

    # Check if required columns exist in both DataFrames
    required_columns_df = ['markets', 'year', 'admissions_final']
    required_columns_class = ['year'] + [col for sublist in thresholds.values() for col in sublist]

    for col in required_columns_df:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in input DataFrame 'df'")

    for col in required_columns_class:
        if col not in classification.columns:
            raise ValueError(f"Column '{col}' not found in classification DataFrame")

    # Create a new column for the classification
    df['classification'] = np.nan

    for index, row in df.iterrows():
        market = row['markets']
        year = row['year']
        admission = row['admissions_final'] # replace admissions_final with admissions_weighted //
                                               # admissions_new_fest factors in festivals

        # Filter the classification DataFrame for the correct year and thresholds
        class_row = classification[classification['year'] == year]

        if not class_row.empty:
            # Get the thresholds for the specific market size
            market_thresholds = thresholds[market]

            # Compare with each threshold
            for threshold in market_thresholds:
                if admission <= class_row.iloc[0][threshold]:
                    df.at[index, 'classification'] = threshold
                    break
            else:
                # If no break occurred, it means admission exceeded all thresholds
                df.at[index, 'classification'] = 'high_threshold'+"_"+market.lower()
    return df

categorized_df_2022 = classify_films2022(df_no_na_2022, classification_2022report)

categorized_df_2022['classification'].value_counts()

Our method

def classify_films(df, classification):
    # Define the threshold names for each market size
    thresholds = {
        'S': ['micro_threshold_s', 'low_threshold_s', 'medium_threshold_s', 'high_threshold_s'],
        'M': ['micro_threshold_m', 'low_threshold_m', 'medium_threshold_m', 'high_threshold_m'],
        'L': ['micro_threshold_l', 'low_threshold_l', 'medium_threshold_l', 'high_threshold_l']
    }

    # Check if required columns exist in both DataFrames
    required_columns_df = ['markets', 'year', 'admissions_final']
    required_columns_class = ['year'] + [col for sublist in thresholds.values() for col in sublist]

    for col in required_columns_df:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in input DataFrame 'df'")

    for col in required_columns_class:
        if col not in classification.columns:
            raise ValueError(f"Column '{col}' not found in classification DataFrame")

    # Create a new column for the classification
    df['classification'] = np.nan

    for index, row in df.iterrows():
        market = row['markets']
        year = row['year']
        admission = row['admissions_final'] # replace admissions_final with admissions_weighted //
                                               # admissions_new_fest factors in festivals

        # Filter the classification DataFrame for the correct year and thresholds
        class_row = classification[classification['year'] == year]

        if not class_row.empty:
            # Get the thresholds for the specific market size
            market_thresholds = thresholds[market]

            # Compare with each threshold
            for threshold in market_thresholds:
                if admission <= class_row.iloc[0][threshold]:
                    df.at[index, 'classification'] = threshold
                    break
            else:
                # If no break occurred, it means admission exceeded all thresholds
                df.at[index, 'classification'] = 'super_high_threshold'+"_"+market.lower()
    return df

categorized_df = classify_films(df_no_na, classification)

values_sample = []
films=[]
total_films=[]

for year in range(2008, 2023):  # Loop from 2008 to 2022
    s = categorized_df.loc[categorized_df['year'] == year].shape[0]
    filtered_df = categorized_df[
        (categorized_df['year'] == year) &
        ((categorized_df['classification'] == 'micro_threshold_l') |
         (categorized_df['classification'] == 'micro_threshold_s') |
         (categorized_df['classification'] == 'micro_threshold_m') |
         (categorized_df['classification'] == 'low_threshold_l') |
         (categorized_df['classification'] == 'low_threshold_s') |
         (categorized_df['classification'] == 'low_threshold_m'))
    ]

    n=filtered_df.shape[0]
    # Append the value to the list
    print(f"Year: {year}, Count: {n}, Sum of Films: {s}, percentage of 0-sucess films {n/s:.2f}, percentage of 1-success films {1-n/s:.2f}")
    value=1-n/s
    films.append(s-n)
    total_films.append(s)

    # Append the value to the list
    values_sample.append(value)
    #years_sample.append(filtered_df.iloc[0]['sum_films'])  # Append the year to the list
mean_value_sample = sum(values_sample) / len(values_sample) if values_sample else 0  # Handle empty list
print(f"Mean value: {mean_value_sample:.2f}")
print(f"N. of films in '1' class: {films}")
print(f"N.of films in sample: {total_films}")



values_sample2022 = []
films2022=[]
total_films2022=[]


for year in range(2008, 2022):  # Loop from 2008 to 2022
    s = categorized_df_2022.loc[categorized_df_2022['year'] == year].shape[0]
    filtered_df = categorized_df_2022[
        (categorized_df_2022['year'] == year) &
        ((categorized_df_2022['classification'] == 'micro_threshold_l') |
         (categorized_df_2022['classification'] == 'micro_threshold_s') |
         (categorized_df_2022['classification'] == 'micro_threshold_m') |
          (categorized_df_2022['classification'] == 'micro_threshold_ml') |
         (categorized_df_2022['classification'] == 'micro_threshold_ss') |
         (categorized_df_2022['classification'] == 'micro_threshold_sm') |
         (categorized_df_2022['classification'] == 'low_threshold_ml') |
         (categorized_df_2022['classification'] == 'low_threshold_s') |
         (categorized_df_2022['classification'] == 'low_threshold_m') |
          (categorized_df_2022['classification'] == 'low_threshold_l') |
         (categorized_df_2022['classification'] == 'low_threshold_ss') |
         (categorized_df_2022['classification'] == 'low_threshold_sm'))
    ]

    n=filtered_df.shape[0]
    # Append the value to the list
    print(f"Year: {year}, Count: {n}, Sum of Films: {s}, percentage of 0-sucess films {n/s:.2f}, percentage of 1-success films {1-n/s:.2f}")
    value=1-n/s
    films2022.append(s-n)
    total_films2022.append(s)

    # Append the value to the list
    values_sample2022.append(value)
    #years_sample.append(filtered_df.iloc[0]['sum_films'])  # Append the year to the list
mean_value_sample2022 = sum(values_sample2022) / len(values_sample2022) if values_sample2022 else 0  # Handle empty list
print(f"Mean value: {mean_value_sample2022:.2f}")
print(f"N. of films in '1' class: {films2022}")
print(f"N.of films in sample: {total_films2022}")

# Graph on the films in our sample
# The number of films is retreived from the full database calculated in the next step. Includes only films with theatrical release.

yearly_production = [1062, 1160, 1218, 1257, 1330, 1422, 1561, 1542, 1548, 1673, 1751, 1768, 1467, 1658, 1730]
years = list(range(2008, 2022))

# Calculate rolling averages with a window size of 3 (can be adjusted)
window_size = 3
rolling_avg_values = pd.Series(values_sample).rolling(window=window_size, min_periods=1).mean().tolist()
rolling_avg_years = pd.Series(yearly_production).rolling(window=window_size, min_periods=1).mean().tolist()
rolling_avg_values2022 = pd.Series(values_sample2022).rolling(window=window_size, min_periods=1).mean().tolist()

# Create Plotly figures

colors = px.colors.sequential.Burgyl
color_for_bars = colors[1]  # Light color for bars
color_for_lines = colors[6]  # Darker color for lines

fig = go.Figure()


# Add trace for rolling average of total number of films
fig.add_trace(go.Bar(
    x=years,
    y=rolling_avg_years,
    name=f'Total N. of Films',
    marker_color= color_for_bars,
    yaxis='y1'
))

# Add trace for rolling average of percentage of 1-success films
fig.add_trace(go.Scatter(
    x=years,
    y=rolling_avg_values,
    mode='lines',
    name=f'Ratio of Most Successful Films',
    line=dict(color=color_for_lines),
    yaxis='y2'
))

# Add trace for rolling average of percentage of 1-success films
fig.add_trace(go.Scatter(
    x=years,
    y=rolling_avg_values2022,
    mode='lines',
    name=f'Ratio of Most Successful Films - 2023 Report',
    line=dict(color=color_for_lines, dash='dash'),
    yaxis='y2'
))


# Update layout with secondary y-axis
fig.update_layout(
    title='Total N. of Films and Ratio of Most Successful Films by Year',
    xaxis_title='Year',
    yaxis_title='Number of Films',
    yaxis=dict(
        title='Total N. of Films',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size=14),
        linewidth=1,
        linecolor='black'),

    yaxis2=dict(
        title='Ratio of Most Successful Films',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size =14),
        overlaying='y',
        side='right',
        linewidth=1,
        linecolor='black'
    ),

    plot_bgcolor='white',  # Remove background color
    paper_bgcolor='white', # Remove outer background color
    legend=dict(x=0, y=0, traceorder='normal', font=dict(size=12)),
    xaxis=dict(
    zerolinecolor='lightgrey',
    titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size =14),
     gridcolor='white',
    linewidth=1,
        linecolor='black'
    ),
    width=800,  # Set the width of the plot
    height=500  # Set the height of the plot

)

# Save the figure to a file
fig.write_image(path+"plotly_graph_sample.pdf")

# Show plot
fig.show()

Repeat the process using all films released between 2003 and 2022 to verify the previous findings

#Define the new market clusters according to the 2023 report (six classes)

country_list_ss = ["EE", "HR", "LT", "LV", "SI", "SK", "BG"]

country_list_s = ["AT", "BE", "CH", "PT", "DK", "GR", "FI", "HU", "RO", "NO", "SE", "CZ", "UA"]

country_list_sm = ["NL"]

country_list_m = ["PL", "TR"]

country_list_ml = ["DE", "IT", "ES"]

country_list_l = ["FR", "GB", "IE"]

obs2022=obs.copy()
obs2022['markets']=np.nan
obs2022

def check_first_country(value):
    first_country = str(value).split(',')[0].strip()  # Get the first country and strip any whitespace

    if first_country in country_list_ss:
        return 'SS'
    elif first_country in country_list_s:
        return 'S'
    elif first_country in country_list_sm:
        return 'SM'
    elif first_country in country_list_m:
        return 'M'
    elif first_country in country_list_ml:
        return 'ML'
    elif first_country in country_list_l:
        return 'L'
    else:
        # Fallback to the value in 'markets' if the country doesn't match any list
        return obs2022['markets'].loc[obs2022['country'] == value].values[0] if value in df['country'].values else np.nan

# Apply the combined function to the 'country' column
obs2022['markets'] = obs2022['country'].apply(check_first_country)


obs2022["markets"].value_counts()

obs2022_no_na=obs2022.dropna(subset=['markets'])
obs2022_no_na= obs2022_no_na.rename(columns={'admissions': 'admissions_final'})

obs_new = obs
obs_new['markets']=np.nan

country_list_s = ["CY", "EE", "FI", "GR", "HR", "IS", "LT", "LU", "LV", "BA", "GE", "ME", "MK", "MT", "SI", "SK", "BG"]
country_list_m = ["AT", "BE", "CH", "PT", "DK", "HU", "IE", "NL", "RO", "NO", "SE", "CZ", "UA"]
country_list_l = ["DE", "FR", "IT", "ES", "GB", "PL"]

def check_first_country(value):
    first_country = str(value).split(',')[0].strip()  # Get the first country and strip any whitespace

    if first_country in country_list_s:
        return 'S'
    elif first_country in country_list_m:
        return 'M'
    elif first_country in country_list_l:
        return 'L'
    else:
        # Fallback to the existing value in 'markets' if no match
        return obs_new['markets'].loc[obs_new['country'] == value].values[0] if value in obs_new['country'].values else np.nan

# Apply the combined function to the 'country' column
obs_new['markets'] = obs_new['country'].apply(check_first_country)


obs_new['markets'].value_counts()

obs_no_na = obs_new.dropna(subset=['markets'])

obs_no_na = obs_no_na.rename(columns={'admissions': 'admissions_final'})

obs_no_na

obs2022_no_na.head(30)

categorized_df_2022 = classify_films2022(obs2022_no_na, classification_2022report)

categorized_df_obs = classify_films(obs_no_na, classification)

categorized_df_2022['classification'].value_counts()

categorized_df_obs['classification'].value_counts()

categorized_df_2022=categorized_df_2022.dropna(subset="classification")
categorized_df_2022

categorized_df_obs=categorized_df_obs.dropna(subset="classification")
categorized_df_obs

values_sample2022 = []
years = list(range(2003, 2022))  # List of years from 2003 to 2021
total_films2022 = []  # To store the total number of films for each year
films2022=[]

for year in years:
    # Count the total number of films for the current year
    s = categorized_df_2022.loc[categorized_df_2022['year'] == year].shape[0]
    total_films2022.append(s)  # Store total films for each year

    # Filter based on classification and year
    filtered_df = categorized_df_2022[
        (categorized_df_2022['year'] == year) &
        (categorized_df_2022['classification'].isin([
            'micro_threshold_l', 'micro_threshold_s', 'micro_threshold_m',
            'micro_threshold_ml', 'micro_threshold_ss', 'micro_threshold_sm',
            'low_threshold_ml', 'low_threshold_s', 'low_threshold_m',
            'low_threshold_l', 'low_threshold_ss', 'low_threshold_sm'
        ]))
    ]

    n = filtered_df.shape[0]

    if s > 0:  # Prevent division by zero
        # Calculate and print statistics
        percentage_0_success = n / s
        percentage_1_success = 1 - percentage_0_success

        print(f"Year: {year}, Count: {n}, Sum of Films: {s}, "
              f"Percentage of 0-success films: {percentage_0_success:.2f}, "
              f"Percentage of 1-success films: {percentage_1_success:.2f}")

        # Append the calculated value to values_sample2022
        value = 1 - percentage_0_success
        values_sample2022.append(value)
        films2022.append(s-n)

    else:
        print(f"Year: {year}, Count: {n}, Sum of Films: {s}, "
              f"Percentage of 0-success films: N/A, "
              f"Percentage of 1-success films: N/A")
        values_sample2022.append(None)  # Or handle as needed

# Calculate the mean value, ignoring None values
cleaned_values_sample2022 = [v for v in values_sample2022 if v is not None]
mean_value_sample2022 = sum(cleaned_values_sample2022) / len(cleaned_values_sample2022) if cleaned_values_sample2022 else 0

print(f"Mean value: {mean_value_sample2022:.2f}")
print(f"N. of films in '1' class: {films2022}")
print(f"N. of films per year : {total_films2022}")

values_sample = []
years = list(range(2003, 2022))  # List of years from 2003 to 2021
total_films = []  # To store the total number of films for each year
films=[]

for year in years:
    # Count the total number of films for the current year
    s = categorized_df_obs.loc[categorized_df_obs['year'] == year].shape[0]
    total_films.append(s)  # Store total films for each year

    filtered_df = categorized_df_obs[
        (categorized_df_obs['year'] == year) &
        ((categorized_df_obs['classification'] == 'micro_threshold_l') |
         (categorized_df_obs['classification'] == 'micro_threshold_s') |
         (categorized_df_obs['classification'] == 'micro_threshold_m') |
         (categorized_df_obs['classification'] == 'low_threshold_l') |
         (categorized_df_obs['classification'] == 'low_threshold_s') |
         (categorized_df_obs['classification'] == 'low_threshold_m'))
    ]

    n = filtered_df.shape[0]
    # Append the value to the list
    print(f"Year: {year}, Count: {n}, Sum of Films: {s}, percentage of 0-success films {n/s:.2f}, percentage of 1-success films {1-n/s:.2f}")

    value = 1 - n / s
    values_sample.append(value)
    films.append(s-n)

    # Check if the filtered_df is empty before accessing its first row


# Ensure years_sample has the same length as years

mean_value_sample = sum(values_sample) / len(values_sample) if values_sample else 0
print(f"Mean value: {mean_value_sample:.2f}")
print(f"N. of films in '1' class: {films}")
print(f"N. of films per year : {total_films}")

### 6.2 - Graphs on Ratio of Success

# Graph with all films in the database with theatrical release (2003-2021)

# Calculate rolling averages with a window size of 3 (can be adjusted)
total_films= [777, 848, 873, 977, 1005, 1062, 1160, 1218, 1257, 1330, 1422, 1561, 1542, 1548, 1673, 1751, 1768, 1467, 1658, 1730]
years = list(range(2003, 2022))  # List of years from 2003 to 2022

window_size = 3
rolling_avg_values = pd.Series(values_sample).rolling(window=window_size, min_periods=1).mean().tolist()
rolling_avg_values2022 = pd.Series(values_sample2022).rolling(window=window_size, min_periods=1).mean().tolist()
rolling_avg_years = pd.Series(total_films).rolling(window=window_size, min_periods=1).mean().tolist()

# Create Plotly figures
colors = px.colors.sequential.Burgyl
color_for_bars = colors[1]  # Light color for bars
color_for_lines = colors[6]  # Darker color for lines

fig = go.Figure()


# Add trace for rolling average of total number of films
fig.add_trace(go.Bar(
    x=years,
    y=rolling_avg_years,
    name=f'Total N. of Films',
    marker_color= color_for_bars,
    yaxis='y1'
))

# Add trace for rolling average of percentage of 1-success films
fig.add_trace(go.Scatter(
    x=years,
    y=rolling_avg_values,
    mode='lines',
    name=f'Ratio of Most Successful Films',
    line=dict(color=color_for_lines),
    yaxis='y2'
))

fig.add_trace(go.Scatter(
    x=years,
    y=rolling_avg_values2022,
    mode='lines',
    name=f'Ratio of Most Successful Films - 2023 Report',
    line=dict(color=color_for_lines, dash='dash'),
    yaxis='y2'
))


# Update layout with secondary y-axis
fig.update_layout(
    title='Total N. of Films and Ratio of Most Successful Films by Year',
    xaxis_title='Year',
    yaxis_title='Number of Films',
    yaxis=dict(
        title='Total N. of Films',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size=14),
        linewidth=1,
        linecolor='black'),

    yaxis2=dict(
        title='Ratio of Most Successful Films',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size =14),
        overlaying='y',
        side='right',
        linewidth=1,
        linecolor='black'
    ),

    plot_bgcolor='white',  # Remove background color
    paper_bgcolor='white', # Remove outer background color
    legend=dict(x=0, y=0, traceorder='normal', font=dict(size=12)),
    xaxis=dict(
    zerolinecolor='lightgrey',
    titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size =14),
     gridcolor='white',
    linewidth=1,
        linecolor='black'
    ),
    width=800,  # Set the width of the plot
    height=500  # Set the height of the plot

)

# Save the figure to a file
fig.write_image(path+"plotly_graph_allfilms.pdf")

# Show plot
fig.show()

### 6.3 - Graph on Diminishing Returns

# Use the values calculated on 6.1

total_films= [777, 848, 873, 977, 1005, 1062, 1160, 1218, 1257, 1330, 1422, 1561, 1542, 1548, 1673, 1751, 1768, 1467, 1658, 1730]
films= [159, 195, 188, 210, 227, 225, 240, 239, 227, 235, 245, 280, 275, 261, 293, 349, 280, 149, 172, 220]
films_sample= [16, 39, 37, 34, 43, 49, 59, 69, 71, 86, 107, 92, 42, 49, 39]
total_films_sample = [57, 103, 91, 95, 142, 198, 224, 285, 337, 417, 471, 520, 435, 465, 341]


films_sample=[12, 23, 14, 18, 25, 28, 23, 30, 33, 41, 43, 38, 23, 20]
total_films_sample= [57, 102, 91, 95, 137, 192, 221, 272, 322, 404, 444, 481, 413, 449]
films= [163, 185, 167, 200, 241, 211, 216, 222, 247, 228, 258, 256, 290, 292, 311, 327, 298, 174, 195]
total_films=[780, 855, 879, 975, 1008, 1062, 1160, 1221, 1265, 1336, 1426, 1566, 1548, 1552, 1678, 1757, 1776, 1500, 1702]

# Given data for our sample
films_produced = total_films_sample
success_values = films_sample
print(films_produced)
print(success_values)

delta_films = np.diff(films_produced)
delta_success = np.diff(success_values)

start_year = 2008
years = np.arange(start_year, start_year + len(films_produced))

# Step 1: Calculate the differences
delta_films = np.diff(films_produced)
delta_success = np.diff(success_values)

# Step 2: Calculate the Marginal Rate of Success (MRS) using difference quotient
mrs_values_sample = delta_films / delta_success

# Corresponding years for MRS
mrs_years = years[1:]  # MRS values correspond to differences between consecutive years

# Print the MRS values
for i, mrs in enumerate(mrs_values_sample):
    print(f"MRS between years {years[i]} and {years[i+1]}: {mrs:.8f}")

# Plotting MRS with years on the x-axis
plt.figure(figsize=(10, 5))
plt.plot(mrs_years, mrs_values_sample, marker='o', linestyle='-', color='green', label="MRS")
plt.axhline(0, color='red', linestyle='--')
plt.title("Marginal Rate of Success (MRS) Between Consecutive Years")
plt.xlabel("Year")
plt.ylabel("MRS (ΔFilms/ΔSuccess)")
plt.xticks(ticks=mrs_years, labels=[f"{year}" for year in mrs_years], rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Given data for all years
films_produced = total_films
success_values = films
print(films_produced)
print(success_values)

delta_films = np.diff(films_produced)
delta_success = np.diff(success_values)

start_year = 2003
years = np.arange(start_year, start_year + len(films_produced))

# Step 1: Calculate the differences
delta_films = np.diff(films_produced)
delta_success = np.diff(success_values)

# Step 2: Calculate the Marginal Rate of Success (MRS) using difference quotient
mrs_values = delta_films / delta_success

# Corresponding years for MRS
mrs_years = years[1:]  # MRS values correspond to differences between consecutive years

# Print the MRS values
for i, mrs in enumerate(mrs_values):
    print(f"MRS between years {years[i]} and {years[i+1]}: {mrs:.8f}")

# Plotting MRS with years on the x-axis
plt.figure(figsize=(10, 5))
plt.plot(mrs_years, mrs_values, marker='o', linestyle='-', color='green', label="MRS")
plt.axhline(0, color='red', linestyle='--')
plt.title("Marginal Rate of Success (MRS) Between Consecutive Years")
plt.xlabel("Year")
plt.ylabel("MRS (ΔFilms/ΔSuccess)")
plt.xticks(ticks=mrs_years, labels=[f"{year}" for year in mrs_years], rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

mrs_values[9:-1]

#Data for all films (LUMIERE)
films_produced = total_films[6:]
success_values = films
mrs_values1 = mrs_values[5:]

print(len(films_produced))
print(len(mrs_values1))

# Define colors using Plotly Express
colors = px.colors.sequential.Burgyl
color_for_bars = colors[1]  # Light color
color_for_lines = colors[6]  # Darker color
color_for_lines2 = colors[3]

# Fit a 2nd degree polynomial to the MRS data
coeffs = np.polyfit(films_produced, mrs_values1, 2)  # 3nd degree polynomial
p = np.poly1d(coeffs)

# Generate x values for the polynomial line (smooth curve)
x_poly = np.linspace(min(films_produced), max(films_produced), 300)
y_poly = p(x_poly)

# Find the x value of the maximum point on the curve
max_y_index = np.argmax(y_poly)  # Index of the maximum y value
max_x_value = x_poly[max_y_index]  # Corresponding x value
max_y_value = y_poly[max_y_index]  # Corresponding y value

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=x_poly,
    y=y_poly,
    mode='lines',
    name='MRS - Polynomial Fit',
    line=dict(color=color_for_bars)
))
fig.add_shape(type="line",
              x0=max_x_value,
              y0=-20,
              x1=max_x_value,
              y1=max_y_value,
              line=dict( width=2, dash="dash", color=color_for_lines2)
              )


fig.add_annotation(x=max_x_value, y=max_y_value, yshift=10,
                   text="Point of Diminishing Returns",
                   font=dict(size=12, color="black"),
                   align="center",
                   opacity=1)

# Update layout
fig.update_layout(
    title='MRS per Yearly Film Production',
    xaxis_title='Films Released',
    yaxis_title='MRS Value',
    yaxis=dict(
        range=[-20, 20],
        title='MRS Value',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size=14),
        linecolor='black'
    ),
    plot_bgcolor='white',  # Remove background color
    paper_bgcolor='white', # Remove outer background color
    legend=dict(x=0, y=0, traceorder='normal', font=dict(size=12)),
    xaxis=dict(
        zerolinecolor='lightgrey',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size=14),
        gridcolor='white',
        linewidth=1,
        linecolor='black'
    ),
    width=800,  # Set the width of the plot
    height=500  # Set the height of the plot
)
fig.write_image(path+"plotly_graph_MRS.pdf")

# Show plot
print(max_y_value, max_x_value)
fig.show()

# Data from our sample
films_produced = total_films[6:-1]
mrs_values1 = mrs_values_sample[:-1]
print(mrs_values1)
print(films_produced)

print(len(films_produced))
print(len(mrs_values1))

# Define colors using Plotly Express
colors = px.colors.sequential.Burgyl
color_for_bars = colors[1]  # Light color for bars
color_for_lines = colors[6]  # Darker color for lines
color_for_lines2 = colors[3]

# Fit a 2nd degree polynomial to the MRS data
coeffs = np.polyfit(films_produced, mrs_values1, 2)  # 3nd degree polynomial
p = np.poly1d(coeffs)

# Generate x values for the polynomial line (smooth curve)
x_poly = np.linspace(min(films_produced), max(films_produced), 300)
y_poly = p(x_poly)

# Find the x value of the maximum point on the polynomial curve
max_y_index = np.argmax(y_poly)  # Index of the maximum y value
max_x_value = x_poly[max_y_index]  # Corresponding x value
max_y_value = y_poly[max_y_index]

fig = go.Figure()

# Add trace for MRS value

# Add trace for the polynomial fit
fig.add_trace(go.Scatter(
    x=x_poly,
    y=y_poly,
    mode='lines',
    name='MRS - Polynomial Fit',
    line=dict(color=color_for_bars)
))
fig.add_shape(type="line",
              x0=max_x_value,
              y0=-20,
              x1=max_x_value,
              y1=max_y_value,
              line=dict( width=2, dash="dash", color=color_for_lines2)
              )

fig.add_annotation(x=max_x_value, y=max_y_value, yshift=10,
                   text="Point of Diminishing Returns",
                   font=dict(size=12, color="black"),
                   align="center",
                   opacity=1)

# Update layout with secondary y-axis
fig.update_layout(
    title='MRS per Yearly Film Production',
    xaxis_title='Films Released',
    yaxis_title='MRS Value',
    yaxis=dict(
        range=[0, 10],
        title='MRS Value',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size=14),
        linecolor='black'
    ),
    plot_bgcolor='white',  # Remove background color
    paper_bgcolor='white', # Remove outer background color
    legend=dict(x=0, y=0, traceorder='normal', font=dict(size=14)),
    xaxis=dict(
        range=[1150, 1690],
        zerolinecolor='lightgrey',
        titlefont=dict(color='black', size=14),
        tickfont=dict(color='black', size=14),
        gridcolor='white',
        linewidth=1,
        linecolor='black'
    ),
    width=800,  # Set the width of the plot
    height=500  # Set the height of the plot
)
fig.write_image(path+"plotly_graph_MRS.pdf")
print(max_x_value, max_y_value)
# Show plot
fig.show()
